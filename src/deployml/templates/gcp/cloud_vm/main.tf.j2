{% if create_artifact_bucket %}
{% for stage in stack %}
  {% for stage_name, tool in stage.items() %}
    {% if tool.params.get("artifact_bucket") %}
resource "google_storage_bucket" "{{ stage_name }}_{{ tool.name }}_artifact" {
  name          = var.artifact_bucket
  location      = var.region
  force_destroy = true

  labels = {
    component  = "{{ tool.name }}-artifacts"
    managed-by = "terraform"
    stage      = "{{ stage_name }}"
  }
}
    {% endif %}
  {% endfor %}
{% endfor %}
{% endif %}

provider "google" {
  project = "{{ project_id }}"
  region  = "{{ region }}"
  zone    = "{{ zone }}"
}

# Detect PostgreSQL and collect MLflow configuration from all stages
{% set flags = namespace(needs_postgres=false, first_tool_name="", mlflow_params={}) %}
{% for stage in stack %}
  {% for stage_name, tool in stage.items() %}
    {% if tool.name == "mlflow" %}
      {% for key, value in tool.params.items() %}
        {% set _ = flags.mlflow_params.update({key: value}) %}
      {% endfor %}
      {% if tool.params.get("backend_store_uri", "") == "postgresql" %}
        {% set flags.needs_postgres = true %}
        {% if not flags.first_tool_name %}
          {% set flags.first_tool_name = tool.name %}
        {% endif %}
      {% endif %}
    {% endif %}
  {% endfor %}
{% endfor %}

# Enable required Google Cloud APIs for fresh project
resource "google_project_service" "required_apis" {
  for_each = toset([
    "compute.googleapis.com",                   # Compute Engine (VMs, disks, networks)
    "storage.googleapis.com",                   # Cloud Storage (artifact buckets)
    "iam.googleapis.com",                       # Identity and Access Management
    "iamcredentials.googleapis.com",            # IAM Service Account Credentials
    "logging.googleapis.com",                   # Cloud Logging
    "monitoring.googleapis.com",                # Cloud Monitoring
    "serviceusage.googleapis.com",              # Service Usage (for enabling APIs)
    "cloudresourcemanager.googleapis.com",     # Cloud Resource Manager (project operations)
    {% if flags.needs_postgres %}
    "sqladmin.googleapis.com",                  # Cloud SQL Admin API (for PostgreSQL)
    "sql-component.googleapis.com",             # Cloud SQL component API
    "servicenetworking.googleapis.com",         # For private service connections
    "cloudkms.googleapis.com",                  # For encryption keys (if using CMEK)
    {% endif %}
  ])

  project = "{{ project_id }}"
  service = each.value

  # Keep APIs enabled when destroying resources
  disable_on_destroy = false
}

# Wait for API propagation (critical for fresh projects)
resource "time_sleep" "wait_for_api_propagation" {
  depends_on = [
    google_project_service.required_apis
  ]

  create_duration = "120s"  # 2 minutes for API propagation (sufficient for most cases)
}

# Simple API readiness marker (no external dependencies)
resource "null_resource" "api_readiness_check" {
  depends_on = [time_sleep.wait_for_api_propagation]
  
  # Simple trigger to ensure proper sequencing
  triggers = {
    api_wait_complete = timestamp()
  }
}

# Create Cloud SQL PostgreSQL instance if needed
{% if flags.needs_postgres %}
# Random password for the database
resource "random_password" "db_password" {
  length  = 16
  special = true
  override_special = "!#$*+-.=_"
}

# Cloud SQL PostgreSQL instance
resource "google_sql_database_instance" "postgres" {
  name             = "{{ flags.first_tool_name }}-postgres-{{ project_id }}"
  database_version = "POSTGRES_14"
  region           = var.region
  project          = var.project_id
  depends_on       = [null_resource.api_readiness_check]

  settings {
    tier = "db-f1-micro"
    ip_configuration {
      authorized_networks {
        value = "0.0.0.0/0"
      }
      ipv4_enabled = true
    }
  }

  deletion_protection = false
}

# Database within the instance
resource "google_sql_database" "db" {
  name     = "{{ flags.first_tool_name }}"
  instance = google_sql_database_instance.postgres.name
  project  = var.project_id
  depends_on = [google_sql_database_instance.postgres]
}

# Database user
resource "google_sql_user" "users" {
  name     = "{{ flags.first_tool_name }}"
  instance = google_sql_database_instance.postgres.name
  password = random_password.db_password.result
  project  = var.project_id
  depends_on = [google_sql_database_instance.postgres]
}

# Note: If terraform destroy fails with "role cann       ot be dropped because some objects depend on it",
# this is because MLflow created tables/objects owned by the user. To fix:
# 1. Connect to the database: gcloud sql connect {{ flags.first_tool_name }}-postgres-{{ project_id }} --user={{ flags.first_tool_name }}
# 2. Run: DROP OWNED BY {{ flags.first_tool_name }} CASCADE;
# 3. Then retry terraform destroy
{% endif %}

# Storage bucket - only create if explicitly requested
resource "google_storage_bucket" "artifact" {
  count         = var.create_bucket && var.artifact_bucket != "" ? 1 : 0
  name          = var.artifact_bucket
  location      = var.region
  force_destroy = true
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  labels = {
    component  = "mlflow-artifacts"
    managed-by = "terraform"
  }
}

# Create a service account for the VM
resource "google_service_account" "vm_service_account" {
  account_id   = "mlflow-vm-sa"
  display_name = "Service Account for MLflow VM"
  project      = var.project_id
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
}

# Project-level IAM bindings for the service account (these will show up in the IAM UI)
{% for stage in stack %}
  {% for stage_name, tool in stage.items() %}
    {% if tool.name == "mlflow" and tool.params.get("artifact_bucket") %}
resource "google_project_iam_member" "vm_service_account_storage_admin" {
  project    = var.project_id
  role       = "roles/storage.objectAdmin"
  member     = "serviceAccount:${google_service_account.vm_service_account.email}"
  depends_on = [google_service_account.vm_service_account, null_resource.api_readiness_check]
}

resource "google_project_iam_member" "vm_service_account_storage_viewer" {
  project    = var.project_id
  role       = "roles/storage.objectViewer"
  member     = "serviceAccount:${google_service_account.vm_service_account.email}"
  depends_on = [google_service_account.vm_service_account, null_resource.api_readiness_check]
}
    {% endif %}
  {% endfor %}
{% endfor %}

# Additional useful permissions for the VM service account
resource "google_project_iam_member" "vm_service_account_logging" {
  project    = var.project_id
  role       = "roles/logging.logWriter"
  member     = "serviceAccount:${google_service_account.vm_service_account.email}"
  depends_on = [google_service_account.vm_service_account, null_resource.api_readiness_check]
}

resource "google_project_iam_member" "vm_service_account_monitoring" {
  project    = var.project_id
  role       = "roles/monitoring.metricWriter"
  member     = "serviceAccount:${google_service_account.vm_service_account.email}"
  depends_on = [google_service_account.vm_service_account, null_resource.api_readiness_check]
}

# Define a Google Compute Engine instance
resource "google_compute_instance" "mlflow_vm" {
  name         = "{{ flags.mlflow_params.vm_name }}"
  machine_type = "{{ flags.mlflow_params.machine_type }}"
  zone         = "{{ zone }}"

  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check{% if flags.needs_postgres %}, google_sql_database_instance.postgres, google_sql_database.db, google_sql_user.users{% endif %}]

  boot_disk {
    initialize_params {
      image = "debian-cloud/debian-12"
      size  = {{ flags.mlflow_params.disk_size_gb }}
      type  = "pd-standard"
    }
  }

  network_interface {
    network = "default"
    access_config {
      // Ephemeral public IP
    }
  }

  # Service account
  service_account {
    email  = google_service_account.vm_service_account.email
    scopes = ["cloud-platform"] # Grant access to Google Cloud APIs
  }

  # Startup script to install Docker and deploy MLflow
  metadata = {
    startup-script = local.default_startup_script
  }

  tags = ["mlflow-server"]

  # Allow stopping for update
  allow_stopping_for_update = true
}

# Local variables for startup script
locals {
  {% if flags.needs_postgres %}
  # Use PostgreSQL connection string from the direct resources
  postgres_connection_string = "postgresql+psycopg2://${google_sql_user.users.name}:${random_password.db_password.result}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  backend_store_uri = "postgresql+psycopg2://${google_sql_user.users.name}:${random_password.db_password.result}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  {% else %}
  # Use SQLite for default backend
  backend_store_uri = "sqlite:///mlflow.db"
  {% endif %}

  default_startup_script = <<-EOF
    #!/bin/bash
    set -e
    
    echo "Starting MLflow VM setup{{ ' with PostgreSQL backend' if flags.needs_postgres else '' }}..."
    
    # Log all output to a file for debugging
    exec > >(tee /var/log/mlflow-startup.log) 2>&1
    
    echo "$(date): Starting MLflow VM setup..."
    
    # Get the current user dynamically
    CURRENT_USER=$(whoami)
    echo "Current user: $CURRENT_USER"
    
    # Update system packages
    echo "Updating system packages..."
    sudo apt-get update -y
    
    # Install necessary packages for Docker and Python
    echo "Installing dependencies..."
    sudo apt-get install -y \
      apt-transport-https \
      ca-certificates \
      curl \
      gnupg \
      lsb-release \
      software-properties-common \
      python3 \
      python3-pip \
      python3-venv \
      python3-dev \
      build-essential \
      git \
      wget \
      unzip{% if flags.needs_postgres %} \
      postgresql-client{% endif %}
    
    # Verify Python and pip are available
    echo "Verifying Python installation..."
    python3 --version
    pip3 --version
    
    # Add Docker's official GPG key
    echo "Adding Docker GPG key..."
    curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
    
    # Set up Docker repository
    echo "Setting up Docker repository..."
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    
    # Update packages and install Docker
    echo "Installing Docker..."
    sudo apt-get update -y
    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin
    
    # Start and enable Docker
    echo "Starting Docker service..."
    sudo systemctl enable docker
    sudo systemctl start docker
    
    # Add current user to docker group
    echo "Configuring Docker permissions for user: $CURRENT_USER"
    sudo usermod -aG docker $CURRENT_USER
    
    # Wait for Docker to be ready
    echo "Waiting for Docker to be ready..."
    sleep 10
    
    # Test Docker installation
    echo "Testing Docker installation..."
    sudo docker run --rm hello-world
    
    # Set up containerized MLflow environment
    echo "Setting up containerized MLflow environment..."
    
    # Create deployment directory structure
    mkdir -p /home/$CURRENT_USER/deployml/docker
    mkdir -p /home/$CURRENT_USER/deployml/docker/mlflow
    mkdir -p /home/$CURRENT_USER/deployml/docker/fastapi
    
    # Create Docker Compose file
    echo "Creating Docker Compose configuration..."
    cat > /home/$CURRENT_USER/deployml/docker/docker-compose.yml << 'DOCKER_COMPOSE_EOF'
version: '3.8'

services:
  mlflow:
    build: 
      context: ./mlflow
      dockerfile: Dockerfile
    container_name: mlflow-server
    ports:
      - "{{ flags.mlflow_params.mlflow_port }}:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI=${local.backend_store_uri}
      - MLFLOW_DEFAULT_ARTIFACT_ROOT={% if flags.mlflow_params.get('artifact_bucket') %}gs://{{ flags.mlflow_params.artifact_bucket }}{% else %}./mlflow-artifacts{% endif %}
      - MLFLOW_SERVER_HOST=0.0.0.0
      - MLFLOW_SERVER_PORT=5000
    volumes:
      - mlflow-data:/app/mlflow-data
      - mlflow-config:/app/mlflow-config
    networks:
      - mlflow-network
    restart: unless-stopped
    command: >
      mlflow server 
      --host 0.0.0.0 
      --port 5000
      --backend-store-uri ${local.backend_store_uri}
      --default-artifact-root {% if flags.mlflow_params.get('artifact_bucket') %}gs://{{ flags.mlflow_params.artifact_bucket }}{% else %}./mlflow-artifacts{% endif %}
    
  fastapi:
    build:
      context: ./fastapi
      dockerfile: Dockerfile
    container_name: fastapi-proxy
    ports:
      - "{{ flags.mlflow_params.get('fastapi_port', 8000) }}:8000"
    environment:
      - MLFLOW_BASE_URL=http://mlflow:5000
      - FASTAPI_PORT=8000
    depends_on:
      - mlflow
    networks:
      - mlflow-network
    restart: unless-stopped

volumes:
  mlflow-data:
  mlflow-config:

networks:
  mlflow-network:
    driver: bridge
DOCKER_COMPOSE_EOF

    # Create MLflow Dockerfile
    echo "Creating MLflow Dockerfile..."
    cat > /home/$CURRENT_USER/deployml/docker/mlflow/Dockerfile << 'MLFLOW_DOCKERFILE_EOF'
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    {% if flags.needs_postgres %}postgresql-client \{% endif %}
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
RUN pip install --upgrade pip setuptools wheel

# Install MLflow and dependencies
RUN pip install \
    mlflow[extras] \
    sqlalchemy \
    {% if flags.needs_postgres %}psycopg2-binary \{% endif %}
    google-cloud-storage \
    boto3

# Create mlflow user
RUN useradd -m -s /bin/bash mlflow

# Create directories
RUN mkdir -p /app/mlflow-data /app/mlflow-config
RUN chown -R mlflow:mlflow /app

# Switch to mlflow user
USER mlflow

# Expose MLflow port
EXPOSE 5000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:5000/health || exit 1

# Default command
CMD ["mlflow", "server", "--host", "0.0.0.0", "--port", "5000"]
MLFLOW_DOCKERFILE_EOF

    # Create FastAPI Dockerfile
    echo "Creating FastAPI Dockerfile..."
    cat > /home/$CURRENT_USER/deployml/docker/fastapi/Dockerfile << 'FASTAPI_DOCKERFILE_EOF'
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
RUN pip install --upgrade pip setuptools wheel

# Install FastAPI and dependencies
RUN pip install \
    fastapi \
    uvicorn \
    httpx

# Create fastapi user
RUN useradd -m -s /bin/bash fastapi

# Create app directory
RUN mkdir -p /app/fastapi-app
RUN chown -R fastapi:fastapi /app

# Copy FastAPI application
COPY main.py /app/fastapi-app/main.py

# Switch to fastapi user
USER fastapi

# Expose FastAPI port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Default command
CMD ["uvicorn", "fastapi-app.main:app", "--host", "0.0.0.0", "--port", "8000"]
FASTAPI_DOCKERFILE_EOF

    # Setup FastAPI application
    echo "Setting up FastAPI application..."
    FASTAPI_SOURCE="{{ flags.mlflow_params.get('fastapi_app_source', 'template') }}"
    
    if [ "$FASTAPI_SOURCE" = "template" ]; then
        echo "Using default containerized FastAPI template..."
        # Create a containerized FastAPI application with MLflow proxy
        cat > /home/$CURRENT_USER/deployml/docker/fastapi/main.py << 'FASTAPI_TEMPLATE_EOF'
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import RedirectResponse, HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
import httpx
import os
from contextlib import asynccontextmanager
import logging
import asyncio

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# MLflow configuration - use container name for inter-container communication
MLFLOW_BASE_URL = os.getenv("MLFLOW_BASE_URL", "http://mlflow:5000")
FASTAPI_PORT = int(os.getenv("FASTAPI_PORT", "8000"))

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan events"""
    logger.info("FastAPI MLflow Proxy starting...")
    logger.info(f"Proxying requests to MLflow at: {MLFLOW_BASE_URL}")
    
    # Wait for MLflow to be ready
    logger.info("Waiting for MLflow to be ready...")
    max_retries = 30
    for i in range(max_retries):
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{MLFLOW_BASE_URL}/health", timeout=5.0)
                if response.status_code == 200:
                    logger.info("✅ MLflow is ready!")
                    break
        except Exception as e:
            logger.info(f"Waiting for MLflow... (attempt {i+1}/{max_retries})")
            if i == max_retries - 1:
                logger.error(f"❌ MLflow not ready after {max_retries} attempts: {e}")
            await asyncio.sleep(2)
    
    yield
    logger.info("FastAPI MLflow Proxy shutting down...")

# Create FastAPI application
app = FastAPI(
    title="MLflow Proxy API",
    description="Containerized FastAPI proxy for MLflow server",
    version="1.0.0",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/", response_class=HTMLResponse)
async def root():
    """Root endpoint with links to available services"""
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>MLflow Proxy API</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            h1 { color: #333; }
            .links { margin: 20px 0; }
            .link { display: block; padding: 10px; margin: 5px 0; background: #f0f0f0; text-decoration: none; border-radius: 5px; }
            .link:hover { background: #e0e0e0; }
            .container-info { background: #e7f3ff; padding: 15px; border-radius: 5px; margin: 20px 0; }
        </style>
    </head>
    <body>
        <h1>🚀 MLflow Proxy API</h1>
        <div class="container-info">
            <h3>🐳 Containerized Deployment</h3>
            <p>This FastAPI proxy is running in a Docker container and communicating with MLflow via Docker networking.</p>
            <p><strong>MLflow URL:</strong> """ + MLFLOW_BASE_URL + """</p>
        </div>
        <div class="links">
            <a class="link" href="/mlflow">📊 MLflow UI</a>
            <a class="link" href="/health">🏥 Health Check</a>
            <a class="link" href="/docs">📚 API Documentation</a>
            <a class="link" href="/container-info">🐳 Container Info</a>
        </div>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{MLFLOW_BASE_URL}/health", timeout=5.0)
            if response.status_code == 200:
                return {
                    "status": "healthy",
                    "mlflow": "connected",
                    "mlflow_url": MLFLOW_BASE_URL,
                    "proxy_port": FASTAPI_PORT,
                    "deployment": "containerized"
                }
            else:
                return {
                    "status": "unhealthy",
                    "mlflow": "disconnected",
                    "mlflow_status_code": response.status_code,
                    "deployment": "containerized"
                }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "mlflow_url": MLFLOW_BASE_URL,
            "deployment": "containerized"
        }

@app.get("/container-info")
async def container_info():
    """Container information endpoint"""
    return {
        "container_name": "fastapi-proxy",
        "mlflow_container": "mlflow-server",
        "mlflow_url": MLFLOW_BASE_URL,
        "network": "mlflow-network",
        "ports": {
            "fastapi": FASTAPI_PORT,
            "mlflow": 5000
        }
    }

@app.get("/mlflow")
async def mlflow_ui_redirect():
    """Redirect to MLflow UI"""
    return RedirectResponse(url=f"{MLFLOW_BASE_URL}/")

@app.get("/mlflow/{path:path}")
async def proxy_mlflow_ui(path: str, request: Request):
    """Proxy MLflow UI requests"""
    try:
        query_params = str(request.url.query)
        url = f"{MLFLOW_BASE_URL}/{path}"
        if query_params:
            url += f"?{query_params}"
        
        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers=dict(request.headers))
            return response.content
    except Exception as e:
        logger.error(f"MLflow UI proxy error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.api_route("/api/2.0/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH"])
async def proxy_mlflow_api(path: str, request: Request):
    """Proxy MLflow API requests"""
    try:
        query_params = str(request.url.query)
        url = f"{MLFLOW_BASE_URL}/api/2.0/{path}"
        if query_params:
            url += f"?{query_params}"
        
        async with httpx.AsyncClient() as client:
            response = await client.request(
                method=request.method,
                url=url,
                headers=dict(request.headers),
                content=await request.body()
            )
            return response.content
    except Exception as e:
        logger.error(f"MLflow API proxy error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=FASTAPI_PORT)
FASTAPI_TEMPLATE_EOF
        echo "✅ Default FastAPI template created successfully!"
    elif [[ "$FASTAPI_SOURCE" == gs://* ]]; then
        echo "Downloading FastAPI application from GCS: $FASTAPI_SOURCE"
        if gsutil cp "$FASTAPI_SOURCE" /home/$CURRENT_USER/deployml/docker/fastapi/main.py; then
            echo "✅ FastAPI application downloaded successfully from GCS!"
        else
            echo "❌ ERROR: Failed to download FastAPI application from GCS: $FASTAPI_SOURCE"
            echo "Please ensure the file exists and you have proper permissions."
            exit 1
        fi
    elif [[ "$FASTAPI_SOURCE" == /* ]]; then
        echo "Copying FastAPI application from local path: $FASTAPI_SOURCE"
        if [ -f "$FASTAPI_SOURCE" ]; then
            cp "$FASTAPI_SOURCE" /home/$CURRENT_USER/deployml/docker/fastapi/main.py
            echo "✅ FastAPI application copied successfully!"
        else
            echo "❌ ERROR: FastAPI application not found at: $FASTAPI_SOURCE"
            echo "Please provide a valid file path or use 'template' for the default application."
            exit 1
        fi
    else
        echo "❌ ERROR: Invalid FastAPI source: $FASTAPI_SOURCE"
        echo "Supported sources:"
        echo "  - 'template' for default FastAPI application"
        echo "  - 'gs://bucket/path/main.py' for GCS file"
        echo "  - '/absolute/path/main.py' for local file"
        exit 1
    fi

    # Set proper permissions
    echo "Setting proper permissions for user: $CURRENT_USER"
    sudo chown -R $CURRENT_USER:$CURRENT_USER /home/$CURRENT_USER/deployml
    
    # Create systemd service for Docker Compose
    echo "Creating Docker Compose systemd service..."
    sudo tee /etc/systemd/system/mlflow-docker.service > /dev/null <<DOCKER_SERVICE_EOF
[Unit]
Description=MLflow Docker Compose Stack
After=network.target docker.service
Requires=docker.service

[Service]
Type=oneshot
RemainAfterExit=true
User=$CURRENT_USER
Group=$CURRENT_USER
WorkingDirectory=/home/$CURRENT_USER/deployml/docker
Environment=MLFLOW_BACKEND_STORE_URI=${local.backend_store_uri}
Environment=MLFLOW_DEFAULT_ARTIFACT_ROOT={% if flags.mlflow_params.get('artifact_bucket') %}gs://{{ flags.mlflow_params.artifact_bucket }}{% else %}./mlflow-artifacts{% endif %}
ExecStart=/usr/bin/docker compose up -d
ExecStop=/usr/bin/docker compose down
ExecReload=/usr/bin/docker compose restart
Restart=no
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
DOCKER_SERVICE_EOF

    # Build Docker containers first
    echo "Building Docker containers..."
    cd /home/$CURRENT_USER/deployml/docker
    sudo -u $CURRENT_USER docker compose build
    
    # Reload systemd and enable Docker Compose service
    echo "Enabling Docker Compose service..."
    sudo systemctl daemon-reload
    sudo systemctl enable mlflow-docker.service
    
    echo "Starting Docker containers via systemd..."
    sudo systemctl start mlflow-docker.service
    
    # Wait for containers to start
    echo "Waiting for containers to start..."
    sleep 30
    
    # Check Docker Compose service status
    echo "Checking Docker Compose service status..."
    sudo systemctl status mlflow-docker --no-pager
    
    # Check container status
    echo "Checking container status..."
    sudo -u $CURRENT_USER docker ps
    
    # Test MLflow container
    echo "Testing MLflow container..."
    for i in {1..10}; do
      if curl -s http://localhost:{{ flags.mlflow_params.mlflow_port }}/health > /dev/null; then
        echo "✅ MLflow container is running successfully!"
        break
      else
        echo "Attempt $i: MLflow container not responding yet..."
        if [ $i -eq 10 ]; then
          echo "⚠️  MLflow container may still be starting up..."
          echo "Checking MLflow container logs..."
          sudo -u $CURRENT_USER docker logs mlflow-server
        fi
        sleep 15
      fi
    done
    
    # Test FastAPI container
    echo "Testing FastAPI container..."
    for i in {1..10}; do
      if curl -s http://localhost:{{ flags.mlflow_params.get('fastapi_port', 8000) }}/health > /dev/null; then
        echo "✅ FastAPI container is running successfully!"
        break
      else
        echo "Attempt $i: FastAPI container not responding yet..."
        if [ $i -eq 10 ]; then
          echo "⚠️  FastAPI container may still be starting up..."
          echo "Checking FastAPI container logs..."
          sudo -u $CURRENT_USER docker logs fastapi-proxy
        fi
        sleep 15
      fi
    done
    
    # Get external IP for display
    EXTERNAL_IP=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip -H "Metadata-Flavor: Google")
    echo "🐳 Containerized MLflow deployment{{ ' with PostgreSQL backend' if flags.needs_postgres else '' }} completed successfully!"
    echo "🌐 MLflow UI will be available at: http://$EXTERNAL_IP:{{ flags.mlflow_params.mlflow_port }}"
    echo "🚀 FastAPI Proxy will be available at: http://$EXTERNAL_IP:{{ flags.mlflow_params.get('fastapi_port', 8000) }}"
    echo "📊 Container Info: http://$EXTERNAL_IP:{{ flags.mlflow_params.get('fastapi_port', 8000) }}/container-info"
    echo "🔧 SSH into the VM with: gcloud compute ssh {{ flags.mlflow_params.vm_name }} --zone={{ zone }}"
    echo "🐳 Manage containers: docker ps, docker logs mlflow-server, docker logs fastapi-proxy"
    echo "🔧 Docker Compose: docker compose up -d, docker compose down, docker compose restart"
    echo "Backend store: {{ 'PostgreSQL' if flags.needs_postgres else 'SQLite' }}"
    {% if flags.mlflow_params.get('artifact_bucket') %}
    echo "Artifact store: gs://{{ flags.mlflow_params.artifact_bucket }}"
    {% endif %}
    
    echo "$(date): VM setup completed successfully!"
    echo "Startup script completed successfully" | sudo tee /var/log/mlflow-startup-complete.log
    
EOF
}

# Firewall rules for MLflow access
resource "google_compute_firewall" "allow_mlflow" {
  name    = "allow-mlflow"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["{{ flags.mlflow_params.mlflow_port }}"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["mlflow-server"]
}

# Firewall rule to allow FastAPI traffic
resource "google_compute_firewall" "allow_fastapi" {
  name    = "allow-fastapi"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["{{ flags.mlflow_params.get('fastapi_port', 8000) }}"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["mlflow-server"]
}

# Firewall rules for HTTP/HTTPS (if needed)
{% if flags.mlflow_params.get("allow_public_access", false) %}
resource "google_compute_firewall" "allow_http_https" {
  name    = "allow-http-https"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["80", "443"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["http-server", "https-server"]
}

resource "google_compute_firewall" "allow_lb_health_checks" {
  name    = "allow-lb-health-checks"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["80", "443"]
  }
  source_ranges = ["130.211.0.0/22", "35.191.0.0/16"]
  target_tags   = ["http-server", "https-server"]
}
{% endif %}

# Outputs
output "vm_external_ip" {
  value = google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip
}

output "mlflow_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.mlflow_port }}"
}

output "service_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.mlflow_port }}"
}

output "fastapi_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.get('fastapi_port', 8000) }}"
}

output "fastapi_health_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.get('fastapi_port', 8000) }}/health"
}

output "container_info_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.get('fastapi_port', 8000) }}/container-info"
}

output "docker_commands" {
  value = {
    check_containers = "docker ps"
    mlflow_logs      = "docker logs mlflow-server"
    fastapi_logs     = "docker logs fastapi-proxy"
    restart_services = "docker compose restart"
    stop_services    = "docker compose down"
    start_services   = "docker compose up -d"
  }
}

output "vm_name" {
  value = "{{ flags.mlflow_params.vm_name }}"
}

output "ssh_command" {
  value = "gcloud compute ssh --zone {{ zone }} {{ flags.mlflow_params.vm_name }}"
}

# Artifact bucket output  
{% if flags.mlflow_params.get("artifact_bucket") %}
output "bucket_name" {
  value = "{{ flags.mlflow_params.artifact_bucket }}"
}
{% endif %}

# Zone output
output "deployment_zone" {
  value = "{{ zone }}"
}

# Debug outputs for IAM troubleshooting
output "service_account_email" {
  description = "Email of the created service account"
  value       = google_service_account.vm_service_account.email
}

output "iam_debug_info" {
  description = "Debug information for IAM configuration"
  value = {
    service_account_id = google_service_account.vm_service_account.account_id
    project_id         = var.project_id
    {% for stage in stack %}
      {% for stage_name, tool in stage.items() %}
        {% if tool.name == "mlflow" and tool.params.get("artifact_bucket") %}
    artifact_bucket    = "{{ tool.params.artifact_bucket }}"
    iam_bindings_created = "yes - storage permissions applied"
        {% endif %}
      {% endfor %}
    {% endfor %}
  }
}



{% if flags.needs_postgres %}
output "instance_connection_name" {
  value = google_sql_database_instance.postgres.connection_name
  description = "Cloud SQL instance connection name"
}

output "postgresql_credentials" {
  value = {
    db_user                  = google_sql_user.users.name
    db_password              = random_password.db_password.result
    db_name                  = google_sql_database.db.name
    db_public_ip             = google_sql_database_instance.postgres.public_ip_address
    instance_connection_name = google_sql_database_instance.postgres.connection_name
    connection_string        = "postgresql+psycopg2://${google_sql_user.users.name}:${random_password.db_password.result}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  }
  sensitive = true
  description = "PostgreSQL database credentials"
}

output "db_connection_string" {
  value = "postgresql+psycopg2://${google_sql_user.users.name}:${random_password.db_password.result}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  sensitive = true
  description = "PostgreSQL connection string"
}
{% endif %}
