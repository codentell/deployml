{% if create_artifact_bucket %}
{% for stage in stack %}
  {% for stage_name, tool in stage.items() %}
    {% if tool.params.get("artifact_bucket") %}
resource "google_storage_bucket" "{{ stage_name }}_{{ tool.name }}_artifact" {
  name          = var.artifact_bucket
  location      = var.region
  force_destroy = true

  labels = {
    component  = "{{ tool.name }}-artifacts"
    managed-by = "terraform"
    stage      = "{{ stage_name }}"
  }
}
    {% endif %}
  {% endfor %}
{% endfor %}
{% endif %}

provider "google" {
  project = "{{ project_id }}"
  region  = "{{ region }}"
  zone    = "{{ zone }}"
}

# Detect PostgreSQL and collect MLflow configuration from all stages
{% set flags = namespace(needs_postgres=false, first_tool_name="", mlflow_params={}) %}
{% for stage in stack %}
  {% for stage_name, tool in stage.items() %}
    {% if tool.name == "mlflow" %}
      {% for key, value in tool.params.items() %}
        {% set _ = flags.mlflow_params.update({key: value}) %}
      {% endfor %}
      {% if tool.params.get("backend_store_uri", "") == "postgresql" %}
        {% set flags.needs_postgres = true %}
        {% if not flags.first_tool_name %}
          {% set flags.first_tool_name = tool.name %}
        {% endif %}
      {% endif %}
    {% endif %}
  {% endfor %}
{% endfor %}

# Enable required Google Cloud APIs for fresh project
resource "google_project_service" "required_apis" {
  for_each = toset([
    "compute.googleapis.com",                   # Compute Engine (VMs, disks, networks)
    "storage.googleapis.com",                   # Cloud Storage (artifact buckets)
    "iam.googleapis.com",                       # Identity and Access Management
    "iamcredentials.googleapis.com",            # IAM Service Account Credentials
    "logging.googleapis.com",                   # Cloud Logging
    "monitoring.googleapis.com",                # Cloud Monitoring
    "serviceusage.googleapis.com",              # Service Usage (for enabling APIs)
    "cloudresourcemanager.googleapis.com",     # Cloud Resource Manager (project operations)
    {% if flags.needs_postgres %}
    "sqladmin.googleapis.com",                  # Cloud SQL Admin API (for PostgreSQL)
    "sql-component.googleapis.com",             # Cloud SQL component API
    "servicenetworking.googleapis.com",         # For private service connections
    "cloudkms.googleapis.com",                  # For encryption keys (if using CMEK)
    {% endif %}
  ])

  project = "{{ project_id }}"
  service = each.value

  # Keep APIs enabled when destroying resources
  disable_on_destroy = false
}

# Wait for API propagation (critical for fresh projects)
resource "time_sleep" "wait_for_api_propagation" {
  depends_on = [
    google_project_service.required_apis
  ]

  create_duration = "180s"  # 3 minutes for API propagation
}

# Verify API readiness before creating resources
resource "null_resource" "api_readiness_check" {
  depends_on = [time_sleep.wait_for_api_propagation]
  
  provisioner "local-exec" {
    command = <<-EOT
      echo "Verifying {{ 'Compute Engine and Cloud SQL APIs' if flags.needs_postgres else 'Compute Engine API' }} are ready..."
      for i in {1..5}; do
        {% if flags.needs_postgres %}
        if gcloud compute zones list --project={{ project_id }} --limit=1 --format="value(name)" 2>/dev/null | grep -q . && \
           gcloud sql instances list --project={{ project_id }} --limit=1 --format="value(name)" 2>/dev/null; then
        {% else %}
        if gcloud compute zones list --project={{ project_id }} --limit=1 --format="value(name)" 2>/dev/null | grep -q .; then
        {% endif %}
          echo "✅ APIs are ready!"
          exit 0
        else
          echo "⏳ Attempt $i: APIs still propagating, waiting 30s..."
          sleep 30
        fi
      done
      echo "❌ API readiness check failed, but continuing..."
      exit 0
    EOT
  }
}

# Create Cloud SQL PostgreSQL instance if needed
{% if flags.needs_postgres %}
# Random password for the database
resource "random_password" "db_password" {
  length  = 16
  special = true
  override_special = "!#$*+-.=_"
}

# Cloud SQL PostgreSQL instance
resource "google_sql_database_instance" "postgres" {
  name             = "{{ flags.first_tool_name }}-postgres-{{ project_id }}"
  database_version = "POSTGRES_14"
  region           = var.region
  project          = var.project_id
  depends_on       = [null_resource.api_readiness_check]

  settings {
    tier = "db-f1-micro"
    ip_configuration {
      authorized_networks {
        value = "0.0.0.0/0"
      }
      ipv4_enabled = true
    }
  }

  deletion_protection = false
}

# Database within the instance
resource "google_sql_database" "db" {
  name     = "{{ flags.first_tool_name }}"
  instance = google_sql_database_instance.postgres.name
  project  = var.project_id
  depends_on = [google_sql_database_instance.postgres]
}

# Database user
resource "google_sql_user" "users" {
  name     = "{{ flags.first_tool_name }}"
  instance = google_sql_database_instance.postgres.name
  password = random_password.db_password.result
  project  = var.project_id
  depends_on = [google_sql_database_instance.postgres]
}

# Note: If terraform destroy fails with "role cann       ot be dropped because some objects depend on it",
# this is because MLflow created tables/objects owned by the user. To fix:
# 1. Connect to the database: gcloud sql connect {{ flags.first_tool_name }}-postgres-{{ project_id }} --user={{ flags.first_tool_name }}
# 2. Run: DROP OWNED BY {{ flags.first_tool_name }} CASCADE;
# 3. Then retry terraform destroy
{% endif %}

# Storage bucket - only create if explicitly requested
resource "google_storage_bucket" "artifact" {
  count         = var.create_bucket && var.artifact_bucket != "" ? 1 : 0
  name          = var.artifact_bucket
  location      = var.region
  force_destroy = true
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  labels = {
    component  = "mlflow-artifacts"
    managed-by = "terraform"
  }
}

# Create a service account for the VM
resource "google_service_account" "vm_service_account" {
  account_id   = "mlflow-vm-sa"
  display_name = "Service Account for MLflow VM"
  project      = var.project_id
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
}

# Define a Google Compute Engine instance
resource "google_compute_instance" "mlflow_vm" {
  name         = "{{ flags.mlflow_params.vm_name }}"
  machine_type = "{{ flags.mlflow_params.machine_type }}"
  zone         = "{{ zone }}"

  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check{% if flags.needs_postgres %}, google_sql_database_instance.postgres, google_sql_database.db, google_sql_user.users{% endif %}]

  boot_disk {
    initialize_params {
      image = "debian-cloud/debian-12"
      size  = {{ flags.mlflow_params.disk_size_gb }}
      type  = "pd-standard"
    }
  }

  network_interface {
    network = "default"
    access_config {
      // Ephemeral public IP
    }
  }

  # Service account
  service_account {
    email  = google_service_account.vm_service_account.email
    scopes = ["cloud-platform"] # Grant access to Google Cloud APIs
  }

  # Startup script to install Docker and deploy MLflow
  metadata = {
    startup-script = local.default_startup_script
  }

  tags = ["mlflow-server"]

  # Allow stopping for update
  allow_stopping_for_update = true
}

# Local variables for startup script
locals {
  {% if flags.needs_postgres %}
  # Use PostgreSQL connection string from the direct resources
  postgres_connection_string = "postgresql+psycopg2://${google_sql_user.users.name}:${random_password.db_password.result}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  backend_store_uri = "postgresql+psycopg2://${google_sql_user.users.name}:${random_password.db_password.result}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  {% else %}
  # Use SQLite for default backend
  backend_store_uri = "sqlite:///mlflow.db"
  {% endif %}

  default_startup_script = <<-EOF
    #!/bin/bash
    set -e
    
    echo "Starting MLflow VM setup{{ ' with PostgreSQL backend' if flags.needs_postgres else '' }}..."
    
    # Log all output to a file for debugging
    exec > >(tee /var/log/mlflow-startup.log) 2>&1
    
    echo "$(date): Starting MLflow VM setup..."
    
    # Get the current user dynamically
    CURRENT_USER=$(whoami)
    echo "Current user: $CURRENT_USER"
    
    # Update system packages
    echo "Updating system packages..."
    sudo apt-get update -y
    
    # Install necessary packages for Docker and Python
    echo "Installing dependencies..."
    sudo apt-get install -y \
      apt-transport-https \
      ca-certificates \
      curl \
      gnupg \
      lsb-release \
      software-properties-common \
      python3 \
      python3-pip \
      python3-venv \
      python3-dev \
      build-essential \
      git \
      wget \
      unzip{% if flags.needs_postgres %} \
      postgresql-client{% endif %}
    
    # Verify Python and pip are available
    echo "Verifying Python installation..."
    python3 --version
    pip3 --version
    
    # Add Docker's official GPG key
    echo "Adding Docker GPG key..."
    curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
    
    # Set up Docker repository
    echo "Setting up Docker repository..."
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    
    # Update packages and install Docker
    echo "Installing Docker..."
    sudo apt-get update -y
    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin
    
    # Start and enable Docker
    echo "Starting Docker service..."
    sudo systemctl enable docker
    sudo systemctl start docker
    
    # Add current user to docker group
    echo "Configuring Docker permissions for user: $CURRENT_USER"
    sudo usermod -aG docker $CURRENT_USER
    
    # Wait for Docker to be ready
    echo "Waiting for Docker to be ready..."
    sleep 10
    
    # Test Docker installation
    echo "Testing Docker installation..."
    sudo docker run --rm hello-world
    
    # Set up MLflow environment
    echo "Setting up MLflow environment..."
    
    # Create a Python virtual environment
    echo "Creating Python virtual environment..."
    python3 -m venv /home/$CURRENT_USER/mlflow-env
    source /home/$CURRENT_USER/mlflow-env/bin/activate
    
    # Upgrade pip in the virtual environment
    echo "Upgrading pip..."
    pip install --upgrade pip setuptools wheel
    
    # Install MLflow and dependencies
    echo "Installing MLflow{{ ' with PostgreSQL support' if flags.needs_postgres else '' }}..."
    pip install mlflow[extras] sqlalchemy{% if flags.needs_postgres %} psycopg2-binary{% endif %}
    
    # Verify MLflow installation
    echo "Verifying MLflow installation..."
    mlflow --version
    
    # Create MLflow configuration directory
    mkdir -p /home/$CURRENT_USER/mlflow-config
    mkdir -p /home/$CURRENT_USER/mlflow-data
    
    # Set up environment variables
    export MLFLOW_SERVER_HOST=0.0.0.0
    export MLFLOW_SERVER_PORT=5000
    export MLFLOW_BACKEND_STORE_URI="${local.backend_store_uri}"
    
         {% if flags.needs_postgres %}
     # Test PostgreSQL connection
     echo "Testing PostgreSQL connection..."
     python3 -c "
import psycopg2
from urllib.parse import urlparse

connection_string = '${local.postgres_connection_string}'
print('Testing connection to PostgreSQL...')
try:
    parsed = urlparse(connection_string)
    conn = psycopg2.connect(
        host=parsed.hostname,
        port=parsed.port or 5432,
        database=parsed.path[1:],  # Remove leading slash
        user=parsed.username,
        password=parsed.password
    )
    conn.close()
    print('PostgreSQL connection successful!')
except Exception as e:
    print(f'PostgreSQL connection failed: {e}')
    exit(1)
"
     {% endif %}
     
     {% if flags.mlflow_params.get('artifact_bucket') %}
     if [ -n "{{ flags.mlflow_params.artifact_bucket }}" ]; then
       export MLFLOW_DEFAULT_ARTIFACT_ROOT=gs://{{ flags.mlflow_params.artifact_bucket }}
     fi
     {% endif %}
    
    # Create systemd service for MLflow
    echo "Creating MLflow systemd service{{ ' with PostgreSQL backend' if flags.needs_postgres else '' }}..."
    sudo tee /etc/systemd/system/mlflow.service > /dev/null <<SERVICE_EOF
[Unit]
Description=MLflow Server{{ ' with PostgreSQL Backend' if flags.needs_postgres else '' }}
After=network.target

[Service]
Type=simple
User=$CURRENT_USER
Group=$CURRENT_USER
WorkingDirectory=/home/$CURRENT_USER
Environment=PATH=/home/$CURRENT_USER/mlflow-env/bin
Environment=MLFLOW_SERVER_HOST=0.0.0.0
Environment=MLFLOW_SERVER_PORT=5000
Environment=MLFLOW_BACKEND_STORE_URI=${local.backend_store_uri}
{% if flags.mlflow_params.get('artifact_bucket') %}
Environment=MLFLOW_DEFAULT_ARTIFACT_ROOT=gs://{{ flags.mlflow_params.artifact_bucket }}
{% endif %}
ExecStart=/home/$CURRENT_USER/mlflow-env/bin/mlflow server --host 0.0.0.0 --port {{ flags.mlflow_params.mlflow_port }} --backend-store-uri ${local.backend_store_uri}
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
SERVICE_EOF
    
    # Set proper permissions
    sudo chown -R $CURRENT_USER:$CURRENT_USER /home/$CURRENT_USER/mlflow-config
    sudo chown -R $CURRENT_USER:$CURRENT_USER /home/$CURRENT_USER/mlflow-data
    
    # Enable and start MLflow service
    echo "Enabling and starting MLflow service..."
    sudo systemctl daemon-reload
    sudo systemctl enable mlflow.service
    sudo systemctl start mlflow.service
    
    # Check service status
    echo "Checking MLflow service status..."
    sudo systemctl status mlflow.service --no-pager
    
    # Wait for MLflow to start
    echo "Waiting for MLflow server to start..."
    sleep 30
    
    # Test MLflow server
    echo "Testing MLflow server..."
    curl -f http://localhost:5000/health || echo "MLflow server health check failed, but continuing..."
    
         echo "MLflow VM setup{{ ' with PostgreSQL backend' if flags.needs_postgres else '' }} completed successfully!"
     echo "MLflow server should be accessible at http://$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip -H 'Metadata-Flavor: Google'):{{ flags.mlflow_params.mlflow_port }}"
     echo "Backend store: {{ 'PostgreSQL' if flags.needs_postgres else 'SQLite' }}"
     {% if flags.mlflow_params.get('artifact_bucket') %}
     echo "Artifact store: gs://{{ flags.mlflow_params.artifact_bucket }}"
     {% endif %}
    
    # Log final status
    echo "$(date): MLflow VM setup completed!"
    
EOF
}

# Firewall rules for MLflow access
resource "google_compute_firewall" "allow_mlflow" {
  name    = "allow-mlflow"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["{{ flags.mlflow_params.mlflow_port }}"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["mlflow-server"]
}

# Firewall rules for HTTP/HTTPS (if needed)
{% if flags.mlflow_params.get("allow_public_access", false) %}
resource "google_compute_firewall" "allow_http_https" {
  name    = "allow-http-https"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["80", "443"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["http-server", "https-server"]
}

resource "google_compute_firewall" "allow_lb_health_checks" {
  name    = "allow-lb-health-checks"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["80", "443"]
  }
  source_ranges = ["130.211.0.0/22", "35.191.0.0/16"]
  target_tags   = ["http-server", "https-server"]
}
{% endif %}

# Outputs
output "vm_external_ip" {
  value = google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip
}

output "mlflow_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.mlflow_port }}"
}

output "service_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ flags.mlflow_params.mlflow_port }}"
}

output "vm_name" {
  value = "{{ flags.mlflow_params.vm_name }}"
}

output "ssh_command" {
  value = "gcloud compute ssh --zone {{ zone }} {{ flags.mlflow_params.vm_name }}"
}

# Artifact bucket output  
{% if flags.mlflow_params.get("artifact_bucket") %}
output "bucket_name" {
  value = "{{ flags.mlflow_params.artifact_bucket }}"
}
{% endif %}

# Zone output
output "deployment_zone" {
  value = "{{ zone }}"
}



{% if flags.needs_postgres %}
output "instance_connection_name" {
  value = google_sql_database_instance.postgres.connection_name
  description = "Cloud SQL instance connection name"
}

output "postgresql_credentials" {
  value = {
    db_user                  = google_sql_user.users.name
    db_password              = random_password.db_password.result
    db_name                  = google_sql_database.db.name
    db_public_ip             = google_sql_database_instance.postgres.public_ip_address
    instance_connection_name = google_sql_database_instance.postgres.connection_name
    connection_string        = "postgresql+psycopg2://${google_sql_user.users.name}:${random_password.db_password.result}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  }
  sensitive = true
  description = "PostgreSQL database credentials"
}

output "db_connection_string" {
  value = "postgresql+psycopg2://${google_sql_user.users.name}:${random_password.db_password.result}@${google_sql_database_instance.postgres.public_ip_address}:5432/${google_sql_database.db.name}"
  sensitive = true
  description = "PostgreSQL connection string"
}
{% endif %}
