{% if create_artifact_bucket %}
{% for stage in stack %}
  {% for stage_name, tool in stage.items() %}
    {% if tool.params.get("artifact_bucket") %}
resource "google_storage_bucket" "{{ stage_name }}_{{ tool.name }}_artifact" {
  name          = var.artifact_bucket
  location      = var.region
  force_destroy = true

  labels = {
    component  = "{{ tool.name }}-artifacts"
    managed-by = "terraform"
    stage      = "{{ stage_name }}"
  }
}
    {% endif %}
  {% endfor %}
{% endfor %}
{% endif %}

provider "google" {
  project = "{{ project_id }}"
  region  = "{{ region }}"
  zone    = "{{ zone }}"
}

# Enable required Google Cloud APIs for fresh project
resource "google_project_service" "required_apis" {
  for_each = toset([
    "compute.googleapis.com",                   # Compute Engine (VMs, disks, networks)
    "storage.googleapis.com",                   # Cloud Storage (artifact buckets)
    "iam.googleapis.com",                       # Identity and Access Management
    "iamcredentials.googleapis.com",            # IAM Service Account Credentials
    "logging.googleapis.com",                   # Cloud Logging
    "monitoring.googleapis.com",                # Cloud Monitoring
    "serviceusage.googleapis.com",              # Service Usage (for enabling APIs)
    "cloudresourcemanager.googleapis.com",     # Cloud Resource Manager (project operations)
  ])

  project = "{{ project_id }}"
  service = each.value

  # Keep APIs enabled when destroying resources
  disable_on_destroy = false
}

# Wait for API propagation (critical for fresh projects)
resource "time_sleep" "wait_for_api_propagation" {
  depends_on = [
    google_project_service.required_apis
  ]

  create_duration = "180s"  # 3 minutes for API propagation
}

# Verify API readiness before creating resources
resource "null_resource" "api_readiness_check" {
  depends_on = [time_sleep.wait_for_api_propagation]
  
  provisioner "local-exec" {
    command = <<-EOT
      echo "Verifying Compute Engine API is ready..."
      for i in {1..5}; do
        if gcloud compute zones list --project={{ project_id }} --limit=1 --format="value(name)" 2>/dev/null | grep -q .; then
          echo "✅ Compute Engine API is ready!"
          exit 0
        else
          echo "⏳ Attempt $i: API still propagating, waiting 30s..."
          sleep 30
        fi
      done
      echo "❌ API readiness check failed, but continuing..."
      exit 0
    EOT
  }
}

# Storage bucket - only create if explicitly requested
resource "google_storage_bucket" "artifact" {
  count         = var.create_bucket && var.artifact_bucket != "" ? 1 : 0
  name          = var.artifact_bucket
  location      = var.region
  force_destroy = true
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  labels = {
    component  = "mlflow-artifacts"
    managed-by = "terraform"
  }
}

# Create a service account for the VM
resource "google_service_account" "vm_service_account" {
  account_id   = "mlflow-vm-sa"
  display_name = "Service Account for MLflow VM"
  project      = var.project_id
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
}

# Define a Google Compute Engine instance
resource "google_compute_instance" "mlflow_vm" {
  name         = "{{ stack[0].experiment_tracking.params.vm_name }}"
  machine_type = "{{ stack[0].experiment_tracking.params.machine_type }}"
  zone         = "{{ zone }}"

  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]

  boot_disk {
    initialize_params {
      image = "debian-cloud/debian-12"
      size  = {{ stack[0].experiment_tracking.params.disk_size_gb }}
      type  = "pd-standard"
    }
  }

  network_interface {
    network = "default"
    access_config {
      // Ephemeral public IP
    }
  }

  # Service account
  service_account {
    email  = google_service_account.vm_service_account.email
    scopes = ["cloud-platform"] # Grant access to Google Cloud APIs
  }

  # Startup script to install Docker and deploy MLflow
  metadata = merge(var.metadata, {
    startup-script = local.default_startup_script
  })

  tags = ["mlflow-server"]

  # Allow stopping for update
  allow_stopping_for_update = true
}

# Local variables for startup script
locals {
  default_startup_script = <<-EOF
    #!/bin/bash
    echo "$(date): Starting MLflow VM setup..."
    
    # Get the current user dynamically
    CURRENT_USER=$(whoami)
    echo "Current user: $CURRENT_USER"
    
    # Update system packages
    echo "Updating system packages..."
    sudo apt-get update -y
    
    # Install necessary packages for Docker and Python
    echo "Installing dependencies..."
    sudo apt-get install -y \
      apt-transport-https \
      ca-certificates \
      curl \
      gnupg \
      lsb-release \
      software-properties-common \
      python3 \
      python3-pip \
      python3-venv \
      python3-dev \
      build-essential \
      git \
      wget \
      unzip
    
    # Verify Python and pip are available
    echo "Verifying Python installation..."
    python3 --version
    pip3 --version
    
    # Add Docker's official GPG key
    echo "Adding Docker GPG key..."
    curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
    
    # Set up Docker repository
    echo "Setting up Docker repository..."
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    
    # Update packages and install Docker
    echo "Installing Docker..."
    sudo apt-get update -y
    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin
    
    # Start and enable Docker
    echo "Starting Docker service..."
    sudo systemctl enable docker
    sudo systemctl start docker
    
    # Add current user to docker group
    echo "Configuring Docker permissions for user: $CURRENT_USER"
    sudo usermod -aG docker $CURRENT_USER
    
    # Wait for Docker to be ready
    echo "Waiting for Docker to be ready..."
    sleep 10
    
    # Test Docker installation
    echo "Testing Docker installation..."
    sudo docker run --rm hello-world
    
    # Set up MLflow environment
    echo "Setting up MLflow environment..."
    
    # Create a Python virtual environment
    echo "Creating Python virtual environment..."
    python3 -m venv /home/$CURRENT_USER/mlflow-env
    source /home/$CURRENT_USER/mlflow-env/bin/activate
    
    # Upgrade pip in the virtual environment
    echo "Upgrading pip..."
    pip install --upgrade pip setuptools wheel
    
    # Install MLflow and dependencies
    echo "Installing MLflow..."
    pip install mlflow[extras] sqlalchemy psycopg2-binary
    
    # Verify MLflow installation
    echo "Verifying MLflow installation..."
    mlflow --version
    
    # Create MLflow configuration directory
    mkdir -p /home/$CURRENT_USER/mlflow-config
    mkdir -p /home/$CURRENT_USER/mlflow-data
    
    # Set up environment variables
    export MLFLOW_SERVER_HOST=0.0.0.0
    export MLFLOW_SERVER_PORT={{ stack[0].experiment_tracking.params.mlflow_port }}
    
    {% if stack[2].model_registry.params.backend_store_uri %}
    if [ -n "{{ stack[2].model_registry.params.backend_store_uri }}" ]; then
      export MLFLOW_BACKEND_STORE_URI={{ stack[2].model_registry.params.backend_store_uri }}
    fi
    {% endif %}
    
    {% if stack[1].artifact_tracking.params.artifact_bucket %}
    if [ -n "{{ stack[1].artifact_tracking.params.artifact_bucket }}" ]; then
      export MLFLOW_DEFAULT_ARTIFACT_ROOT=gs://{{ stack[1].artifact_tracking.params.artifact_bucket }}
    fi
    {% endif %}
    
    # Create systemd service for MLflow
    echo "Creating MLflow systemd service..."
    sudo tee /etc/systemd/system/mlflow.service > /dev/null <<SERVICE_EOF
[Unit]
Description=MLflow Server
After=network.target

[Service]
Type=simple
User=$CURRENT_USER
Group=$CURRENT_USER
WorkingDirectory=/home/$CURRENT_USER
Environment=PATH=/home/$CURRENT_USER/mlflow-env/bin
Environment=MLFLOW_SERVER_HOST=0.0.0.0
Environment=MLFLOW_SERVER_PORT={{ stack[0].experiment_tracking.params.mlflow_port }}
{% if stack[2].model_registry.params.backend_store_uri %}
Environment=MLFLOW_BACKEND_STORE_URI={{ stack[2].model_registry.params.backend_store_uri }}
{% endif %}
{% if stack[1].artifact_tracking.params.artifact_bucket %}
Environment=MLFLOW_DEFAULT_ARTIFACT_ROOT=gs://{{ stack[1].artifact_tracking.params.artifact_bucket }}
{% endif %}
ExecStart=/home/$CURRENT_USER/mlflow-env/bin/mlflow server --host 0.0.0.0 --port {{ stack[0].experiment_tracking.params.mlflow_port }}
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal
SERVICE_EOF

    # Reload systemd and start MLflow service
    echo "Starting MLflow service..."
    sudo systemctl daemon-reload
    sudo systemctl enable mlflow
    sudo systemctl start mlflow
    
    echo "MLflow VM setup complete!"
    echo "MLflow should be accessible at: http://$(curl -s ifconfig.me):{{ stack[0].experiment_tracking.params.mlflow_port }}"
  EOF
}

# Firewall rules for MLflow access
resource "google_compute_firewall" "allow_mlflow" {
  name    = "allow-mlflow"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["{{ stack[0].experiment_tracking.params.mlflow_port }}"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["mlflow-server"]
}

# Firewall rules for HTTP/HTTPS (if needed)
{% if stack[0].experiment_tracking.params.allow_public_access %}
resource "google_compute_firewall" "allow_http_https" {
  name    = "allow-http-https"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["80", "443"]
  }
  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["http-server", "https-server"]
}

resource "google_compute_firewall" "allow_lb_health_checks" {
  name    = "allow-lb-health-checks"
  network = "default"
  
  # Explicit dependency to ensure APIs are ready FIRST
  depends_on = [null_resource.api_readiness_check]
  
  allow {
    protocol = "tcp"
    ports    = ["80", "443"]
  }
  source_ranges = ["130.211.0.0/22", "35.191.0.0/16"]
  target_tags   = ["http-server", "https-server"]
}
{% endif %}

# Outputs
output "vm_external_ip" {
  value = google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip
}

output "mlflow_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ stack[0].experiment_tracking.params.mlflow_port }}"
}

output "service_url" {
  value = "http://${google_compute_instance.mlflow_vm.network_interface[0].access_config[0].nat_ip}:{{ stack[0].experiment_tracking.params.mlflow_port }}"
}

output "bucket_name" {
  value = "{{ stack[1].artifact_tracking.params.artifact_bucket }}"
}

output "vm_name" {
  value = "{{ stack[0].experiment_tracking.params.vm_name }}"
}

output "zone" {
  value = "{{ zone }}"
}

output "ssh_command" {
  value = "gcloud compute ssh --zone {{ zone }} {{ stack[0].experiment_tracking.params.vm_name }}"
}
